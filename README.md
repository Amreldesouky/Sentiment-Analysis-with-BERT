# Sentiment Analysis with BERT

## ğŸ“Œ Project Overview

This project implements **Sentiment Analysis** using **BERT (Bidirectional Encoder Representations from Transformers)**, a state-of-the-art deep learning model for **natural language processing (NLP)**. Our model classifies text reviews into sentiment categories, providing insights into user opinions with high accuracy.

## ğŸš€ Features

- **Pre-trained BERT Model**: Utilizes `bert-base-uncased` for powerful contextual understanding.
- **Handles Various Sentiments**: giving score from 1 to 5 according to Satisfaction level. 
- **Efficient Tokenization**: Uses the Hugging Face `transformers` library for text preprocessing.
- **CUDA Support**: Runs efficiently on GPUs for faster inference.

## ğŸ“‚ Project Structure

## ğŸ”§ Installation

1. Clone the repository:
2. Create a virtual environment and install dependencies:

## ğŸ› ï¸ Usage

### 2ï¸âƒ£ Running Inference


## âš¡ Performance Insights

- The model performed very well and much better than the clients scores records .
- Leveraging **BERTâ€™s contextual embeddings** significantly improved classification for complex reviews.
- GPU acceleration led to **faster inference times**, reducing prediction latency.

## ğŸ—ï¸ Future Improvements

- **Expand Dataset**: Train on a larger and more diverse dataset for improved generalization.

## ğŸ’¡ Technologies Used

- **Python** ğŸ
- **PyTorch** ğŸ”¥
- **Transformers (Hugging Face)** ğŸ¤—
- **BERT** ğŸ§ 
- **CUDA (GPU Acceleration)** âš¡

## ğŸ¤ Contributing

Contributions are welcome! Feel free to fork the repo and submit pull requests.


---

**Developed by [Amr Khaled Eldesouky]** âœ¨



